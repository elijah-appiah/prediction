---
title: "Machine Learning Prediction Course Project"
author: "Elijah Appiah"
date: "January 1, 2022"
output:
  html_document:
    keep_md: yes
    toc: yes
---

# Machine Learning Preliminary Summary

The goal of your project is to predict the manner in which they did the exercise. This is the “classe” variable in the training set. You may use any of the other variables to predict with. You should create a report describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did. You will also use your prediction model to predict 20 different test cases.

# Task Requirement
Apply your machine learning algorithm to the 20 test cases available in the test data above and submit your predictions in appropriate format to the Course Project Prediction Quiz for automated grading.

#### Data

The training data for this project is available here:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv>

The test data are available here:
<https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv>

The data for this project come from this source: <http://groupware.les.inf.puc-rio.br/har.>

Let us load some packages needed for our machine learning algorithms

```{r}
library(ggplot2)
library(caret)
library(randomForest)
library(rattle)
```

Loading and Checking Dimension of Data

```{r}
trainP <- read.csv("pml-training.csv", na.strings = c("NA", ""))
testingP <-read.csv("pml-testing.csv", na.strings = c("NA", ""))

dim(trainP); dim(testingP)
```

# Data Preprocessing

Data cleaning includes 3 steps as below:

1. Remove columns with NA values.

2. Remove columns with no variations.

3. Remove columns with no relationship with the prediction.

```{r}
# columns with NA count > 0 will be captured in dataNA
dataNA <- colSums(is.na(trainP)) > 0 

# remove the NA columns from training set
trainP1 <- trainP[, !dataNA]

#Remove columns with no variations
NZV <- nearZeroVar(trainP1, saveMetrics = TRUE)
trainP2 <- trainP1[, !NZV$nzv]

# Remove columns with no relationship with the prediction
trainP3 <- trainP2[, -c(1, 2, 3, 4, 5, 6)]
```

# Data Slicing

It is necessary that we split the training data into training set and validation set.

```{r}
inTrain <- createDataPartition(y=trainP3$classe, p=0.7, list=FALSE)
validation <- trainP3[-inTrain,]
training <- trainP3[inTrain,]

dim(training)
```

```{r}
dim(validation)
```

# Modeling

The training set we are using has a lot of data, many variables, and the possibility of noise in the data. Random forest may be the best method to use.

```{r}
tc <- trainControl(method = "cv", 3)

modFitP <- train(classe ~., 
                 method = "rf", 
                 data = training, 
                 trControl = tc, 
                 allowParallel=TRUE, 
                 importance=TRUE, 
                 ntree = 250)

modFitP
```

Predicting our model and generating the confusion matrix yields:

```{r}
prdval <- predict(modFitP, validation)

validation$classe <- as.factor(validation$classe)

confusionMatrix(validation$classe, prdval)
```

The accuracy of the model is also found to be:

```{r}
accuracy <- postResample(prdval, validation$classe)
outsamperr <- 1 - as.numeric(confusionMatrix(validation$classe,prdval)$overall[1])

print(accuracy); print(outsamperr)
```

Thus, it is seen that the accuracy is 99.39% and out of sample error rate is 0.61%.

# Variable Importance

```{r}
varImpt <- varImp(modFitP)$importance
head(varImpt)
```

# Plotting the Accuracy

```{r}
qplot(classe, 
      prdval, 
      data=validation,  
      colour= classe, 
      geom = c("boxplot", "jitter"), 
      main = "predicted vs. observed in validation data", 
      xlab = "Observed Classe", 
      ylab = "Predicted Classe")
```

# Apply model to testing set

```{r}
prdvalT <- predict(modFitP, testingP)

prdvalT
```

